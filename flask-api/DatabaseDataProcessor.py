# # flask-api/DatabaseDataProcessor.py
# import pymysql
# import redis
# import pandas as pd
# from sqlalchemy import create_engine, text
# import json
# import os
# from datetime import datetime, timedelta
# import numpy as np
# from sklearn.preprocessing import LabelEncoder, StandardScaler
# from sklearn.feature_extraction.text import TfidfVectorizer
# from sklearn.decomposition import LatentDirichletAllocation
# import re
# import warnings
# warnings.filterwarnings('ignore')
#
# # NLP ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬
# try:
#     from konlpy.tag import Okt, Mecab
#     KONLPY_AVAILABLE = True
# except ImportError:
#     print("âš ï¸ KoNLPyê°€ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. ê¸°ë³¸ í…ìŠ¤íŠ¸ ì²˜ë¦¬ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
#     KONLPY_AVAILABLE = False
#
# try:
#     from wordcloud import WordCloud
#     WORDCLOUD_AVAILABLE = True
# except ImportError:
#     print("âš ï¸ WordCloudê°€ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. ì›Œë“œí´ë¼ìš°ë“œ ê¸°ëŠ¥ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
#     WORDCLOUD_AVAILABLE = False
#
# class SonStarDataProcessor:
#     def __init__(self, mysql_config=None, redis_config=None):
#         # Config ì„í¬íŠ¸ë¥¼ ì—¬ê¸°ì„œ ìˆ˜í–‰ (ìˆœí™˜ ì°¸ì¡° ë°©ì§€)
#         try:
#             from config import config
#             self.config = config
#         except ImportError:
#             print("âš ï¸ config ëª¨ë“ˆì„ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ ì„¤ì •ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
#             self.config = None
#
#         # ê¸°ë³¸ Docker í™˜ê²½ ì„¤ì •
#         if mysql_config is None:
#             if self.config:
#                 mysql_config = {
#                     'host': self.config.MYSQL_HOST,
#                     'port': self.config.MYSQL_PORT,
#                     'user': self.config.MYSQL_USER,
#                     'password': self.config.MYSQL_PASSWORD,
#                     'database': self.config.MYSQL_DATABASE,
#                     'charset': 'utf8mb4'
#                 }
#             else:
#                 mysql_config = {
#                     'host': 'localhost',
#                     'port': 3306,
#                     'user': 'root',
#                     'password': 'hi092787!!!',
#                     'database': 'SonStar',
#                     'charset': 'utf8mb4'
#                 }
#
#         if redis_config is None:
#             if self.config:
#                 redis_config = {
#                     'host': self.config.REDIS_HOST,
#                     'port': self.config.REDIS_PORT,
#                     'db': self.config.REDIS_DB,
#                     'decode_responses': True
#                 }
#             else:
#                 redis_config = {
#                     'host': 'localhost',
#                     'port': 6379,
#                     'db': 0,
#                     'decode_responses': True
#                 }
#
#         # MySQL ì—°ê²°
#         try:
#             self.mysql_engine = create_engine(
#                 f"mysql+pymysql://{mysql_config['user']}:{mysql_config['password']}@"
#                 f"{mysql_config['host']}:{mysql_config['port']}/{mysql_config['database']}"
#             )
#             print("âœ… MySQL ì—°ê²° ì„±ê³µ")
#         except Exception as e:
#             print(f"âŒ MySQL ì—°ê²° ì‹¤íŒ¨: {e}")
#             self.mysql_engine = None
#
#         # Redis ì—°ê²°
#         try:
#             self.redis_client = redis.Redis(
#                 host=redis_config['host'],
#                 port=redis_config['port'],
#                 db=redis_config['db'],
#                 decode_responses=True
#             )
#             # ì—°ê²° í…ŒìŠ¤íŠ¸
#             self.redis_client.ping()
#             print("âœ… Redis ì—°ê²° ì„±ê³µ")
#         except Exception as e:
#             print(f"âŒ Redis ì—°ê²° ì‹¤íŒ¨: {e}")
#             self.redis_client = None
#
#         # ì¸ì½”ë” ì´ˆê¸°í™”
#         self.label_encoders = {}
#         self.scaler = StandardScaler()
#
#         # NLP ê´€ë ¨ ì´ˆê¸°í™”
#         self.init_nlp_components()
#
#     def init_nlp_components(self):
#         """NLP ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”"""
#         # í•œêµ­ì–´ í˜•íƒœì†Œ ë¶„ì„ê¸° ì´ˆê¸°í™”
#         if KONLPY_AVAILABLE:
#             try:
#                 self.mecab = Mecab()
#                 self.nlp_engine = 'mecab'
#                 print("âœ… Mecab í˜•íƒœì†Œ ë¶„ì„ê¸° ì´ˆê¸°í™” ì™„ë£Œ")
#             except:
#                 try:
#                     self.okt = Okt()
#                     self.nlp_engine = 'okt'
#                     print("âœ… Okt í˜•íƒœì†Œ ë¶„ì„ê¸° ì´ˆê¸°í™” ì™„ë£Œ")
#                 except:
#                     self.nlp_engine = 'basic'
#                     print("âš ï¸ í˜•íƒœì†Œ ë¶„ì„ê¸° ì´ˆê¸°í™” ì‹¤íŒ¨. ê¸°ë³¸ ì²˜ë¦¬ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
#         else:
#             self.nlp_engine = 'basic'
#
#         # í•œêµ­ì–´ ë¶ˆìš©ì–´ ë¦¬ìŠ¤íŠ¸
#         self.korean_stopwords = {
#             'ì´', 'ê·¸', 'ì €', 'ê²ƒ', 'ìˆ˜', 'ë“¤', 'ë“±', 'ë°', 'ë˜í•œ', 'ê·¸ë¦¬ê³ ', 'í•˜ì§€ë§Œ', 'ê·¸ëŸ¬ë‚˜',
#             'ë•Œë¬¸', 'ìœ„í•´', 'í†µí•´', 'ëŒ€í•´', 'ìˆëŠ”', 'ì—†ëŠ”', 'ê°™ì€', 'ë‹¤ë¥¸', 'ë§ì€', 'ì‘ì€', 'í°',
#             'ì¢‹ì€', 'ë‚˜ìœ', 'ìƒˆë¡œìš´', 'ì˜¤ë˜ëœ', 'ì´ëŸ°', 'ì €ëŸ°', 'ê·¸ëŸ°', 'ì–´ë–¤', 'ëª¨ë“ ', 'ê°',
#             'ëª‡', 'ì—¬ëŸ¬', 'ë‹¤ì–‘í•œ', 'íŠ¹ë³„í•œ', 'ì¼ë°˜ì ì¸', 'ê¸°ë³¸ì ì¸', 'ì£¼ìš”', 'ì¤‘ìš”í•œ', 'í•„ìš”í•œ',
#             'ê°€ëŠ¥í•œ', 'ë¶ˆê°€ëŠ¥í•œ', 'ìµœê³ ', 'ìµœì €', 'ìµœëŒ€', 'ìµœì†Œ', 'í•­ìƒ', 'ì ˆëŒ€', 'ê°€ë”', 'ë•Œë•Œë¡œ'
#         }
#
#         # íŒ¨ì…˜/ì‡¼í•‘ ê´€ë ¨ ë¶ˆìš©ì–´
#         self.fashion_stopwords = {
#             'ìƒí’ˆ', 'ì œí’ˆ', 'ì•„ì´í…œ', 'ì˜·', 'ì˜ë¥˜', 'íŒ¨ì…˜', 'ìŠ¤íƒ€ì¼', 'ë””ìì¸', 'ë¸Œëœë“œ',
#             'ì‚¬ì´ì¦ˆ', 'ì»¬ëŸ¬', 'ìƒ‰ìƒ', 'ì†Œì¬', 'ì¬ì§ˆ', 'ì›ë‹¨', 'ê°€ê²©', 'í• ì¸', 'ì„¸ì¼',
#             'ë¬´ë£Œ', 'ë°°ì†¡', 'ë‹¹ì¼', 'ë¹ ë¥¸', 'ì¦‰ì‹œ', 'ë°”ë¡œ', 'ì‹ ìƒ', 'ì¸ê¸°', 'ë² ìŠ¤íŠ¸',
#             'ì¶”ì²œ', 'íŠ¹ê°€', 'í•œì •', 'ì´ë²¤íŠ¸', 'ì¿ í°', 'ì ë¦½', 'í˜œíƒ', 'ì„œë¹„ìŠ¤'
#         }
#
#         self.all_stopwords = self.korean_stopwords.union(self.fashion_stopwords)
#
#         # TF-IDF ë²¡í„°ë¼ì´ì € ì´ˆê¸°í™”
#         self.tfidf_vectorizer = TfidfVectorizer(
#             max_features=1000,
#             min_df=2,
#             max_df=0.8,
#             ngram_range=(1, 2),
#             stop_words=None  # í•œêµ­ì–´ëŠ” ì§ì ‘ ì²˜ë¦¬
#         )
#
#     def extract_korean_keywords(self, text, top_k=10):
#         """í•œêµ­ì–´ í…ìŠ¤íŠ¸ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ"""
#         if not text or pd.isna(text):
#             return []
#
#         # í…ìŠ¤íŠ¸ ì •ì œ
#         text = str(text).strip()
#         if not text:
#             return []
#
#         # íŠ¹ìˆ˜ë¬¸ì ë° ìˆ«ì ì œê±° (í•œê¸€, ì˜ë¬¸, ê³µë°±ë§Œ ìœ ì§€)
#         text = re.sub(r'[^ê°€-í£a-zA-Z\s]', ' ', text)
#         text = re.sub(r'\s+', ' ', text)
#
#         if self.nlp_engine == 'mecab':
#             return self._extract_keywords_mecab(text, top_k)
#         elif self.nlp_engine == 'okt':
#             return self._extract_keywords_okt(text, top_k)
#         else:
#             return self._extract_keywords_basic(text, top_k)
#
#     def _extract_keywords_mecab(self, text, top_k):
#         """Mecabì„ ì‚¬ìš©í•œ í‚¤ì›Œë“œ ì¶”ì¶œ"""
#         try:
#             # í˜•íƒœì†Œ ë¶„ì„ (ëª…ì‚¬, í˜•ìš©ì‚¬, ì˜ì–´ ë‹¨ì–´ë§Œ ì¶”ì¶œ)
#             morphs = self.mecab.pos(text)
#             keywords = []
#
#             for word, pos in morphs:
#                 if (pos.startswith('NN') or pos.startswith('VA') or pos.startswith('SL')) and len(word) > 1:
#                     if word.lower() not in self.all_stopwords:
#                         keywords.append(word.lower())
#
#             # ë¹ˆë„ìˆ˜ ê³„ì‚°
#             from collections import Counter
#             word_freq = Counter(keywords)
#             return [word for word, freq in word_freq.most_common(top_k)]
#
#         except Exception as e:
#             print(f"Mecab í‚¤ì›Œë“œ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
#             return self._extract_keywords_basic(text, top_k)
#
#     def _extract_keywords_okt(self, text, top_k):
#         """Oktë¥¼ ì‚¬ìš©í•œ í‚¤ì›Œë“œ ì¶”ì¶œ"""
#         try:
#             # ëª…ì‚¬ ì¶”ì¶œ
#             nouns = self.okt.nouns(text)
#             # í˜•ìš©ì‚¬ ì¶”ì¶œ
#             adjectives = [word for word, pos in self.okt.pos(text) if pos == 'Adjective']
#
#             keywords = []
#             for word in nouns + adjectives:
#                 if len(word) > 1 and word.lower() not in self.all_stopwords:
#                     keywords.append(word.lower())
#
#             # ë¹ˆë„ìˆ˜ ê³„ì‚°
#             from collections import Counter
#             word_freq = Counter(keywords)
#             return [word for word, freq in word_freq.most_common(top_k)]
#
#         except Exception as e:
#             print(f"Okt í‚¤ì›Œë“œ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
#             return self._extract_keywords_basic(text, top_k)
#
#     def _extract_keywords_basic(self, text, top_k):
#         """ê¸°ë³¸ í‚¤ì›Œë“œ ì¶”ì¶œ (í˜•íƒœì†Œ ë¶„ì„ê¸° ì—†ì´)"""
#         # ë‹¨ìˆœ ê³µë°± ê¸°ë°˜ ë‹¨ì–´ ë¶„ë¦¬
#         words = text.lower().split()
#         keywords = []
#
#         for word in words:
#             # ê¸¸ì´ 2 ì´ìƒ, ë¶ˆìš©ì–´ê°€ ì•„ë‹Œ ë‹¨ì–´ë§Œ
#             if len(word) > 1 and word not in self.all_stopwords:
#                 keywords.append(word)
#
#         # ë¹ˆë„ìˆ˜ ê³„ì‚°
#         from collections import Counter
#         word_freq = Counter(keywords)
#         return [word for word, freq in word_freq.most_common(top_k)]
#
#     def extract_tfidf_keywords(self, texts, top_k=10):
#         """TF-IDFë¥¼ ì‚¬ìš©í•œ í‚¤ì›Œë“œ ì¶”ì¶œ"""
#         try:
#             # ì „ì²˜ë¦¬ëœ í…ìŠ¤íŠ¸ë¡œ TF-IDF ê³„ì‚°
#             processed_texts = []
#             for text in texts:
#                 keywords = self.extract_korean_keywords(text, top_k=50)
#                 processed_texts.append(' '.join(keywords))
#
#             # ë¹ˆ í…ìŠ¤íŠ¸ ì²˜ë¦¬
#             processed_texts = [text if text.strip() else 'ë¹ˆí…ìŠ¤íŠ¸' for text in processed_texts]
#
#             # TF-IDF ê³„ì‚°
#             tfidf_matrix = self.tfidf_vectorizer.fit_transform(processed_texts)
#             feature_names = self.tfidf_vectorizer.get_feature_names_out()
#
#             # ê° ë¬¸ì„œë³„ ìƒìœ„ í‚¤ì›Œë“œ ì¶”ì¶œ
#             doc_keywords = []
#             for doc_idx in range(tfidf_matrix.shape[0]):
#                 tfidf_scores = tfidf_matrix[doc_idx].toarray()[0]
#                 word_scores = list(zip(feature_names, tfidf_scores))
#                 word_scores.sort(key=lambda x: x[1], reverse=True)
#
#                 # ìƒìœ„ í‚¤ì›Œë“œë§Œ ì„ íƒ
#                 keywords = [word for word, score in word_scores[:top_k] if score > 0]
#                 doc_keywords.append(keywords)
#
#             return doc_keywords
#
#         except Exception as e:
#             print(f"TF-IDF í‚¤ì›Œë“œ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
#             # ê¸°ë³¸ í‚¤ì›Œë“œ ì¶”ì¶œë¡œ fallback
#             return [self.extract_korean_keywords(text, top_k) for text in texts]
#
#     def analyze_category_keywords(self, items_df):
#         """ì¹´í…Œê³ ë¦¬ë³„ í‚¤ì›Œë“œ ë¶„ì„"""
#         print("\n=== ì¹´í…Œê³ ë¦¬ë³„ í‚¤ì›Œë“œ ë¶„ì„ ì‹œì‘ ===")
#
#         category_keywords = {}
#
#         for category in items_df['category'].unique():
#             if pd.isna(category):
#                 continue
#
#             category_items = items_df[items_df['category'] == category]
#             category_texts = []
#
#             for _, item in category_items.iterrows():
#                 text = f"{item['item_name']} {item.get('description', '')}"
#                 category_texts.append(text)
#
#             # ì¹´í…Œê³ ë¦¬ë³„ í‚¤ì›Œë“œ ì¶”ì¶œ
#             if category_texts:
#                 keywords = self.extract_tfidf_keywords(category_texts, top_k=15)
#                 # ëª¨ë“  í‚¤ì›Œë“œë¥¼ í•©ì¹˜ê³  ë¹ˆë„ìˆ˜ ê³„ì‚°
#                 all_keywords = [kw for doc_kws in keywords for kw in doc_kws]
#
#                 from collections import Counter
#                 keyword_freq = Counter(all_keywords)
#                 category_keywords[category] = dict(keyword_freq.most_common(10))
#
#                 print(f"  {category}: {list(keyword_freq.keys())[:5]}")
#
#         return category_keywords
#
#     def extract_search_insights(self, redis_data):
#         """ê²€ìƒ‰ í‚¤ì›Œë“œ ì¸ì‚¬ì´íŠ¸ ì¶”ì¶œ"""
#         search_keywords = redis_data.get('search_keywords', [])
#
#         if not search_keywords:
#             return {}
#
#         # ê²€ìƒ‰ í‚¤ì›Œë“œ ë¶„ì„
#         keyword_insights = {
#             'popular_searches': [],
#             'trending_keywords': [],
#             'search_patterns': {}
#         }
#
#         # ì¸ê¸° ê²€ìƒ‰ì–´ (ë¹ˆë„ìˆ˜ ê¸°ì¤€)
#         keyword_insights['popular_searches'] = search_keywords[:20]
#
#         # ê²€ìƒ‰ì–´ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ
#         search_texts = [kw[0] for kw in search_keywords]
#         extracted_keywords = []
#
#         for search_text in search_texts:
#             keywords = self.extract_korean_keywords(search_text, top_k=3)
#             extracted_keywords.extend(keywords)
#
#         # íŠ¸ë Œë”© í‚¤ì›Œë“œ
#         from collections import Counter
#         keyword_freq = Counter(extracted_keywords)
#         keyword_insights['trending_keywords'] = keyword_freq.most_common(15)
#
#         print(f"âœ… ì¸ê¸° ê²€ìƒ‰ì–´: {len(search_keywords)}ê°œ ë¶„ì„ ì™„ë£Œ")
#
#         return keyword_insights
#
#     def check_table_schema(self, table_name):
#         """í…Œì´ë¸” ìŠ¤í‚¤ë§ˆ í™•ì¸"""
#         try:
#             with self.mysql_engine.connect() as conn:
#                 result = conn.execute(text(f"DESCRIBE {table_name}"))
#                 columns = [row[0] for row in result.fetchall()]
#                 print(f"ğŸ“‹ {table_name} í…Œì´ë¸” ì»¬ëŸ¼: {columns}")
#                 return columns
#         except Exception as e:
#             print(f"âŒ {table_name} í…Œì´ë¸” ìŠ¤í‚¤ë§ˆ í™•ì¸ ì‹¤íŒ¨: {e}")
#             return []
#
#     def extract_data_from_mysql(self):
#         """MySQLì—ì„œ SonStar ì‡¼í•‘ëª° ë°ì´í„° ì¶”ì¶œ (ìŠ¤í‚¤ë§ˆ í™•ì¸ í›„)"""
#
#         if self.mysql_engine is None:
#             print("MySQL ì—°ê²°ì´ ì—†ìŠµë‹ˆë‹¤.")
#             return None, None, None
#
#         print("ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆ í™•ì¸ ì¤‘...")
#
#         # ì£¼ìš” í…Œì´ë¸”ë“¤ì˜ ìŠ¤í‚¤ë§ˆ í™•ì¸
#         cart_columns = self.check_table_schema('cart')
#         cart_item_columns = self.check_table_schema('cart_item')
#         orders_columns = self.check_table_schema('orders')
#         wish_list_columns = self.check_table_schema('wish_list')
#
#         # 1. ìƒí’ˆ ë°ì´í„° (Item ì—”í‹°í‹° ê¸°ë°˜)
#         items_query = """
#         SELECT
#             i.id as item_id,
#             i.item_name,
#             i.price,
#             i.gender,
#             i.category,
#             i.sub_category,
#             i.age_group,
#             i.style,
#             i.season,
#             i.image_url,
#             i.item_comment as description,
#             i.quantity,
#             i.item_rating,
#             i.review_count,
#             i.view_count,
#             i.order_count,
#             i.cart_count,
#             i.created_at,
#             i.updated_at
#         FROM item i
#         WHERE i.delete_type != 'Y' OR i.delete_type IS NULL
#         """
#
#         # 2. ì‚¬ìš©ì ë°ì´í„° (Member ì—”í‹°í‹° ê¸°ë°˜)
#         members_query = """
#         SELECT
#             m.id as member_id,
#             m.email,
#             m.name,
#             m.nick_name,
#             m.gender,
#             m.role,
#             m.login_type,
#             m.point,
#             m.used_point,
#             m.created_at as signup_date,
#             m.updated_at as last_activity
#         FROM member m
#         WHERE m.delete_type != 'Y' OR m.delete_type IS NULL
#         """
#
#         # 3. ìƒí˜¸ì‘ìš© ë°ì´í„° (ìŠ¤í‚¤ë§ˆì— ë”°ë¼ ë™ì ìœ¼ë¡œ êµ¬ì„±)
#         interaction_queries = []
#
#         # ì£¼ë¬¸ ë°ì´í„°
#         if 'created_at' in orders_columns:
#             orders_query = """
#             SELECT
#                 o.member_id as user_id,
#                 od.item_id as product_id,
#                 'purchase' as action_type,
#                 o.created_at as timestamp,
#                 od.quantity,
#                 od.price as unit_price,
#                 NULL as session_id
#             FROM orders o
#             JOIN order_detail od ON o.id = od.order_id
#             WHERE o.created_at >= DATE_SUB(NOW(), INTERVAL 6 MONTH)
#             """
#             interaction_queries.append(orders_query)
#
#         # ì¥ë°”êµ¬ë‹ˆ ë°ì´í„° (created_at ì»¬ëŸ¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸)
#         cart_timestamp_col = None
#         if 'created_at' in cart_columns:
#             cart_timestamp_col = 'c.created_at'
#         elif 'updated_at' in cart_columns:
#             cart_timestamp_col = 'c.updated_at'
#         elif 'created_at' in cart_item_columns:
#             cart_timestamp_col = 'ci.created_at'
#         elif 'updated_at' in cart_item_columns:
#             cart_timestamp_col = 'ci.updated_at'
#         else:
#             cart_timestamp_col = 'NOW()'  # ê¸°ë³¸ê°’ìœ¼ë¡œ í˜„ì¬ ì‹œê°„ ì‚¬ìš©
#
#         cart_query = f"""
#         SELECT
#             c.member_id as user_id,
#             ci.item_id as product_id,
#             'cart' as action_type,
#             {cart_timestamp_col} as timestamp,
#             ci.quantity,
#             NULL as unit_price,
#             NULL as session_id
#         FROM cart c
#         JOIN cart_item ci ON c.id = ci.cart_id
#         """
#         interaction_queries.append(cart_query)
#
#         # ìœ„ì‹œë¦¬ìŠ¤íŠ¸ ë°ì´í„°
#         if 'created_at' in wish_list_columns:
#             wish_query = """
#             SELECT
#                 w.member_id as user_id,
#                 w.item_id as product_id,
#                 'like' as action_type,
#                 w.created_at as timestamp,
#                 1 as quantity,
#                 NULL as unit_price,
#                 NULL as session_id
#             FROM wish_list w
#             WHERE w.created_at >= DATE_SUB(NOW(), INTERVAL 6 MONTH)
#             """
#             interaction_queries.append(wish_query)
#
#         # UNION ALLë¡œ í•©ì¹˜ê¸°
#         interactions_query = " UNION ALL ".join(interaction_queries) if interaction_queries else ""
#
#         print("MySQLì—ì„œ ë°ì´í„° ì¶”ì¶œ ì¤‘...")
#         try:
#             items_df = pd.read_sql(items_query, self.mysql_engine)
#             members_df = pd.read_sql(members_query, self.mysql_engine)
#
#             if interactions_query:
#                 interactions_df = pd.read_sql(interactions_query, self.mysql_engine)
#             else:
#                 # ë¹ˆ DataFrame ìƒì„±
#                 interactions_df = pd.DataFrame(columns=['user_id', 'product_id', 'action_type', 'timestamp', 'quantity', 'unit_price', 'session_id'])
#
#             print(f"âœ… ìƒí’ˆ ë°ì´í„°: {len(items_df)}ê°œ")
#             print(f"âœ… íšŒì› ë°ì´í„°: {len(members_df)}ê°œ")
#             print(f"âœ… ìƒí˜¸ì‘ìš© ë°ì´í„°: {len(interactions_df)}ê°œ")
#
#             return items_df, members_df, interactions_df
#
#         except Exception as e:
#             print(f"âŒ ë°ì´í„° ì¶”ì¶œ ì‹¤íŒ¨: {e}")
#             print(f"ì‹¤í–‰í•œ ì¿¼ë¦¬:")
#             print(f"Items: {items_query}")
#             print(f"Members: {members_query}")
#             if interactions_query:
#                 print(f"Interactions: {interactions_query}")
#             return None, None, None
#
#     def extract_data_from_redis(self):
#         """Redisì—ì„œ ì‹¤ì‹œê°„ í–‰ë™ ë°ì´í„° ì¶”ì¶œ"""
#
#         if self.redis_client is None:
#             print("Redis ì—°ê²°ì´ ì—†ìŠµë‹ˆë‹¤.")
#             return {}
#
#         try:
#             # ìµœê·¼ ì¡°íšŒ ìƒí’ˆ (ì‚¬ìš©ìë³„)
#             recent_views = {}
#             view_keys = self.redis_client.keys("member:*:recent_views")
#
#             for key in view_keys:
#                 member_id = key.split(':')[1]
#                 views = self.redis_client.lrange(key, 0, -1)
#                 if views:
#                     recent_views[member_id] = [json.loads(view) for view in views]
#
#             # ì‹¤ì‹œê°„ ì¸ê¸° ìƒí’ˆ (ì¡°íšŒìˆ˜ ê¸°ë°˜)
#             popular_items = self.redis_client.zrevrange("popular_items", 0, 100, withscores=True)
#
#             # ì‹¤ì‹œê°„ ì¥ë°”êµ¬ë‹ˆ ë°ì´í„°
#             cart_data = {}
#             cart_keys = self.redis_client.keys("cart:member:*")
#
#             for key in cart_keys:
#                 member_id = key.split(':')[2]
#                 cart_items = self.redis_client.hgetall(key)
#                 if cart_items:
#                     cart_data[member_id] = cart_items
#
#             # ê²€ìƒ‰ í‚¤ì›Œë“œ ë¡œê·¸
#             search_keywords = self.redis_client.zrevrange("search_keywords", 0, 100, withscores=True)
#
#             print(f"âœ… Redis ì¡°íšŒ ë°ì´í„°: {len(recent_views)}ëª…")
#             print(f"âœ… ì¸ê¸° ìƒí’ˆ: {len(popular_items)}ê°œ")
#             print(f"âœ… ì¥ë°”êµ¬ë‹ˆ ë°ì´í„°: {len(cart_data)}ëª…")
#
#             return {
#                 'recent_views': recent_views,
#                 'popular_items': popular_items,
#                 'cart_data': cart_data,
#                 'search_keywords': search_keywords
#             }
#
#         except Exception as e:
#             print(f"âŒ Redis ë°ì´í„° ì¶”ì¶œ ì‹¤íŒ¨: {e}")
#             return {}
#
#     def merge_mysql_redis_data(self, mysql_data, redis_data):
#         """MySQLê³¼ Redis ë°ì´í„° í†µí•©"""
#         items_df, members_df, interactions_df = mysql_data
#
#         if not redis_data:
#             return items_df, members_df, interactions_df
#
#         # Redis ìµœê·¼ ì¡°íšŒ ë°ì´í„°ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜
#         redis_interactions = []
#         for member_id, views in redis_data.get('recent_views', {}).items():
#             for view in views:
#                 redis_interactions.append({
#                     'user_id': int(member_id),
#                     'product_id': view.get('item_id'),
#                     'action_type': 'view',
#                     'timestamp': datetime.fromtimestamp(view.get('timestamp', 0)),
#                     'quantity': 1,
#                     'unit_price': None,
#                     'session_id': view.get('session_id')
#                 })
#
#         if redis_interactions:
#             redis_df = pd.DataFrame(redis_interactions)
#             # MySQL ë°ì´í„°ì™€ í†µí•© (ì¤‘ë³µ ì œê±°)
#             interactions_df = pd.concat([interactions_df, redis_df]).drop_duplicates(
#                 subset=['user_id', 'product_id', 'action_type'], keep='last'
#             )
#             print(f"âœ… Redis ë°ì´í„° í†µí•©: {len(redis_interactions)}ê°œ ì¶”ê°€")
#
#         return items_df, members_df, interactions_df
#
#     def preprocess_items_data(self, items_df):
#         """ìƒí’ˆ ë°ì´í„° ì „ì²˜ë¦¬ (NLP í‚¤ì›Œë“œ ì¶”ì¶œ í¬í•¨)"""
#         print("\n=== ìƒí’ˆ ë°ì´í„° ì „ì²˜ë¦¬ ì‹œì‘ ===")
#
#         if items_df.empty:
#             print("ìƒí’ˆ ë°ì´í„°ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
#             return items_df
#
#         # 1. ê²°ì¸¡ê°’ ì²˜ë¦¬
#         items_df['description'] = items_df['description'].fillna('')
#         items_df['image_url'] = items_df['image_url'].fillna('')
#         items_df['item_rating'] = items_df['item_rating'].fillna(0)
#         items_df['review_count'] = items_df['review_count'].fillna(0)
#         items_df['view_count'] = items_df['view_count'].fillna(0)
#         items_df['order_count'] = items_df['order_count'].fillna(0)
#         items_df['cart_count'] = items_df['cart_count'].fillna(0)
#
#         # 2. ê°€ê²© ì´ìƒì¹˜ ì²˜ë¦¬
#         items_df['price'] = items_df['price'].clip(lower=1000, upper=10000000)
#
#         # 3. ì¸ê¸°ë„ ì ìˆ˜ ê³„ì‚°
#         items_df['popularity_score'] = (
#                 items_df['view_count'] * 1 +
#                 items_df['cart_count'] * 3 +
#                 items_df['order_count'] * 5 +
#                 items_df['item_rating'] * items_df['review_count'] * 2
#         )
#
#         # 4. ê°€ê²©ëŒ€ ë¶„ë¥˜
#         items_df['price_range'] = pd.cut(
#             items_df['price'],
#             bins=[0, 30000, 80000, 150000, float('inf')],
#             labels=['ì €ê°€', 'ì¤‘ê°€', 'ê³ ê°€', 'í”„ë¦¬ë¯¸ì—„']
#         )
#
#         # 5. ì¹´í…Œê³ ë¦¬ ë°ì´í„° ì¸ì½”ë”©
#         categorical_columns = ['gender', 'category', 'sub_category', 'age_group', 'style', 'season']
#
#         for col in categorical_columns:
#             if col in items_df.columns:
#                 # NULL ê°’ì„ 'UNKNOWN'ìœ¼ë¡œ ì²˜ë¦¬
#                 items_df[col] = items_df[col].fillna('UNKNOWN').astype(str)
#
#                 le = LabelEncoder()
#                 items_df[f'{col}_encoded'] = le.fit_transform(items_df[col])
#                 self.label_encoders[col] = le
#
#         # 6. NLP í‚¤ì›Œë“œ ì¶”ì¶œ ë° í…ìŠ¤íŠ¸ íŠ¹ì„± ì²˜ë¦¬
#         print(" NLP í‚¤ì›Œë“œ ì¶”ì¶œ ì¤‘...")
#
#         # ìƒí’ˆëª…ê³¼ ì„¤ëª… ê²°í•©
#         items_df['combined_text'] = (
#                 items_df['item_name'].fillna('') + ' ' +
#                 items_df['description'].fillna('')
#         )
#
#         # ê°œë³„ ìƒí’ˆë³„ í‚¤ì›Œë“œ ì¶”ì¶œ
#         items_df['keywords'] = items_df['combined_text'].apply(
#             lambda x: self.extract_korean_keywords(x, top_k=8)
#         )
#
#         # í‚¤ì›Œë“œë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜ (ì €ì¥ìš©)
#         items_df['keywords_str'] = items_df['keywords'].apply(
#             lambda x: ','.join(x) if x else ''
#         )
#
#         # TF-IDF ê¸°ë°˜ ìƒí’ˆë³„ ì£¼ìš” í‚¤ì›Œë“œ ì¶”ì¶œ
#         print("TF-IDF ê¸°ë°˜ í‚¤ì›Œë“œ ë¶„ì„ ì¤‘...")
#         tfidf_keywords = self.extract_tfidf_keywords(items_df['combined_text'].tolist(), top_k=5)
#         items_df['tfidf_keywords'] = tfidf_keywords
#         items_df['tfidf_keywords_str'] = items_df['tfidf_keywords'].apply(
#             lambda x: ','.join(x) if x else ''
#         )
#
#         # í…ìŠ¤íŠ¸ ê¸¸ì´ íŠ¹ì„±
#         items_df['text_length'] = items_df['combined_text'].str.len()
#         items_df['keyword_count'] = items_df['keywords'].apply(len)
#
#         # ë¸Œëœë“œëª… ì¶”ì¶œ (ìƒí’ˆëª…ì—ì„œ)
#         items_df['brand'] = items_df['item_name'].apply(self._extract_brand_name)
#
#         print(f"âœ… NLP í‚¤ì›Œë“œ ì¶”ì¶œ ì™„ë£Œ")
#         print(f"ìƒí’ˆ ë°ì´í„° ì „ì²˜ë¦¬ ì™„ë£Œ: {len(items_df)}ê°œ")
#         return items_df
#
#     def _extract_brand_name(self, item_name):
#         """ìƒí’ˆëª…ì—ì„œ ë¸Œëœë“œëª… ì¶”ì¶œ (ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹±)"""
#         if not item_name or pd.isna(item_name):
#             return 'UNKNOWN'
#
#         # ì²« ë²ˆì§¸ ë‹¨ì–´ë¥¼ ë¸Œëœë“œë¡œ ê°€ì • (ê°œì„  ê°€ëŠ¥)
#         words = str(item_name).split()
#         if words:
#             # ì˜ì–´ë¡œ ëœ ì²« ë²ˆì§¸ ë‹¨ì–´ë‚˜ ëŒ€ë¬¸ìë¡œ ì‹œì‘í•˜ëŠ” ë‹¨ì–´ë¥¼ ë¸Œëœë“œë¡œ ê°„ì£¼
#             for word in words:
#                 if re.match(r'^[A-Z][a-zA-Z]+$', word) and len(word) > 2:
#                     return word
#             # ê·¸ ì™¸ì—ëŠ” ì²« ë²ˆì§¸ ë‹¨ì–´
#             return words[0]
#         return 'UNKNOWN'
#
#     def preprocess_members_data(self, members_df):
#         """íšŒì› ë°ì´í„° ì „ì²˜ë¦¬"""
#         print("\n=== íšŒì› ë°ì´í„° ì „ì²˜ë¦¬ ì‹œì‘ ===")
#
#         if members_df.empty:
#             print("íšŒì› ë°ì´í„°ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
#             return members_df
#
#         # 1. ê²°ì¸¡ê°’ ì²˜ë¦¬
#         members_df['point'] = members_df['point'].fillna(0)
#         members_df['used_point'] = members_df['used_point'].fillna(0)
#
#         # 2. ê°€ì… ê¸°ê°„ ê³„ì‚°
#         members_df['signup_date'] = pd.to_datetime(members_df['signup_date'])
#         members_df['days_since_signup'] = (
#                 datetime.now() - members_df['signup_date']
#         ).dt.days
#
#         # 3. ì‚¬ìš©ì ë“±ê¸‰ ë¶„ë¥˜
#         members_df['user_grade'] = pd.cut(
#             members_df['used_point'],
#             bins=[0, 10000, 50000, 100000, float('inf')],
#             labels=['ë¸Œë¡ ì¦ˆ', 'ì‹¤ë²„', 'ê³¨ë“œ', 'ë‹¤ì´ì•„ëª¬ë“œ']
#         )
#
#         # 4. í™œë™ ê¸°ê°„ ë¶„ë¥˜
#         members_df['user_segment'] = pd.cut(
#             members_df['days_since_signup'],
#             bins=[0, 30, 90, 365, float('inf')],
#             labels=['ì‹ ê·œ', 'í™œì„±', 'ê¸°ì¡´', 'ì¥ê¸°']
#         )
#
#         # 5. ì¹´í…Œê³ ë¦¬ ì¸ì½”ë”©
#         categorical_columns = ['gender', 'role', 'login_type', 'user_grade', 'user_segment']
#
#         for col in categorical_columns:
#             if col in members_df.columns:
#                 if pd.api.types.is_categorical_dtype(members_df[col]):
#                     if 'UNKNOWN' not in members_df[col].cat.categories:
#                         members_df[col] = members_df[col].cat.add_categories('UNKNOWN')
#                     members_df[col] = members_df[col].fillna('UNKNOWN')
#                 else:
#                     members_df[col] = members_df[col].fillna('UNKNOWN').astype(str)
#
#                 le = LabelEncoder()
#                 members_df[f'{col}_encoded'] = le.fit_transform(members_df[col])
#                 self.label_encoders[f'member_{col}'] = le
#
#         print(f"íšŒì› ë°ì´í„° ì „ì²˜ë¦¬ ì™„ë£Œ: {len(members_df)}ê°œ")
#         return members_df
#
#     def preprocess_interactions_data(self, interactions_df):
#         """ìƒí˜¸ì‘ìš© ë°ì´í„° ì „ì²˜ë¦¬"""
#         print("\n=== ìƒí˜¸ì‘ìš© ë°ì´í„° ì „ì²˜ë¦¬ ì‹œì‘ ===")
#
#         if interactions_df.empty:
#             print("ìƒí˜¸ì‘ìš© ë°ì´í„°ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
#             # ë¹ˆ DataFrame ë°˜í™˜
#             user_item_matrix = pd.DataFrame(columns=['user_id', 'product_id', 'final_score', 'interaction_count'])
#             return interactions_df, user_item_matrix
#
#         # 1. ì¤‘ë³µ ì œê±°
#         interactions_df = interactions_df.drop_duplicates(
#             subset=['user_id', 'product_id', 'action_type'], keep='last'
#         )
#
#         # 2. í–‰ë™ ê°€ì¤‘ì¹˜ ë¶€ì—¬
#         action_weights = {
#             'view': 1,
#             'like': 2,
#             'cart': 3,
#             'purchase': 5
#         }
#         interactions_df['weight'] = interactions_df['action_type'].map(action_weights)
#
#         # 3. ì‹œê°„ ê¸°ë°˜ ê°€ì¤‘ì¹˜ (ìµœê·¼ í–‰ë™ì¼ìˆ˜ë¡ ë†’ì€ ê°€ì¤‘ì¹˜)
#         interactions_df['timestamp'] = pd.to_datetime(interactions_df['timestamp'])
#         max_date = interactions_df['timestamp'].max()
#         interactions_df['days_ago'] = (max_date - interactions_df['timestamp']).dt.days
#         interactions_df['time_weight'] = np.exp(-interactions_df['days_ago'] / 30)  # 30ì¼ ë°˜ê°ê¸°
#
#         # 4. êµ¬ë§¤ ê¸ˆì•¡ ê³ ë ¤ (êµ¬ë§¤ í–‰ë™ì˜ ê²½ìš°)
#         interactions_df['purchase_value'] = (
#                 interactions_df['quantity'].fillna(1) *
#                 interactions_df['unit_price'].fillna(0)
#         )
#
#         # 5. ìµœì¢… ì ìˆ˜ ê³„ì‚°
#         interactions_df['final_score'] = (
#                 interactions_df['weight'] *
#                 interactions_df['time_weight'] *
#                 (1 + interactions_df['purchase_value'] / 100000)  # êµ¬ë§¤ê¸ˆì•¡ ë³´ì •
#         )
#
#         # 6. ì‚¬ìš©ì-ì•„ì´í…œ ë§¤íŠ¸ë¦­ìŠ¤ ìƒì„±
#         user_item_matrix = interactions_df.groupby(['user_id', 'product_id']).agg({
#             'final_score': 'sum',
#             'action_type': 'count'
#         }).rename(columns={'action_type': 'interaction_count'}).reset_index()
#
#         print(f"ìƒí˜¸ì‘ìš© ë°ì´í„° ì „ì²˜ë¦¬ ì™„ë£Œ: {len(interactions_df)}ê°œ")
#         print(f"ì‚¬ìš©ì-ì•„ì´í…œ ë§¤íŠ¸ë¦­ìŠ¤: {len(user_item_matrix)}ê°œ")
#
#         return interactions_df, user_item_matrix
#
#     def generate_summary_statistics(self, items_df, members_df, interactions_df):
#         """ë°ì´í„° ìš”ì•½ í†µê³„ ìƒì„± (NLP ë¶„ì„ ê²°ê³¼ í¬í•¨)"""
#         print("\n" + "="*60)
#         print("           SonStar ì‡¼í•‘ëª° ë°ì´í„° ìš”ì•½ ë¦¬í¬íŠ¸")
#         print("="*60)
#
#         # ìƒí’ˆ í†µê³„
#         print(f"\n[ìƒí’ˆ í˜„í™©]")
#         print(f"  ì´ ìƒí’ˆ ìˆ˜: {len(items_df):,}ê°œ")
#         if not items_df.empty:
#             print(f"  í‰ê·  ê°€ê²©: {items_df['price'].mean():,.0f}ì›")
#             print(f"  ê°€ê²© ë²”ìœ„: {items_df['price'].min():,} ~ {items_df['price'].max():,}ì›")
#             print(f"  ì¹´í…Œê³ ë¦¬ ìˆ˜: {items_df['category'].nunique()}ê°œ")
#             print(f"  í‰ê·  í‰ì : {items_df['item_rating'].mean():.2f}")
#
#             # í‚¤ì›Œë“œ ë¶„ì„ ê²°ê³¼
#             if 'keyword_count' in items_df.columns:
#                 print(f"  í‰ê·  í‚¤ì›Œë“œ ìˆ˜: {items_df['keyword_count'].mean():.1f}ê°œ")
#                 print(f"  í‰ê·  í…ìŠ¤íŠ¸ ê¸¸ì´: {items_df['text_length'].mean():.0f}ì")
#
#             # ì¹´í…Œê³ ë¦¬ë³„ ìƒí’ˆ ë¶„í¬
#             category_dist = items_df['category'].value_counts()
#             print(f"\n[ì¹´í…Œê³ ë¦¬ë³„ ìƒí’ˆ ë¶„í¬]")
#             for cat, count in category_dist.head().items():
#                 print(f"  {cat}: {count}ê°œ ({count/len(items_df)*100:.1f}%)")
#
#         # íšŒì› í†µê³„
#         print(f"\n[íšŒì› í˜„í™©]")
#         print(f"  ì´ íšŒì› ìˆ˜: {len(members_df):,}ëª…")
#         if not members_df.empty and 'gender' in members_df.columns:
#             gender_dist = members_df['gender'].value_counts()
#             print(f"  ì„±ë³„ ë¶„í¬: {dict(gender_dist)}")
#
#         # ìƒí˜¸ì‘ìš© í†µê³„
#         print(f"\n[í™œë™ í˜„í™©]")
#         print(f"  ì´ ìƒí˜¸ì‘ìš©: {len(interactions_df):,}ê°œ")
#         if not interactions_df.empty:
#             print(f"  í™œì„± ì‚¬ìš©ì: {interactions_df['user_id'].nunique():,}ëª…")
#             print(f"  ìƒí˜¸ì‘ìš©ëœ ìƒí’ˆ: {interactions_df['product_id'].nunique():,}ê°œ")
#
#             action_dist = interactions_df['action_type'].value_counts()
#             print(f"\n[í–‰ë™ ìœ í˜•ë³„ ë¶„í¬]")
#             for action, count in action_dist.items():
#                 print(f"  {action}: {count:,}ê°œ ({count/len(interactions_df)*100:.1f}%)")
#
#         # NLP ë¶„ì„ ê²°ê³¼
#         if not items_df.empty and 'keywords' in items_df.columns:
#             print(f"\n[NLP í‚¤ì›Œë“œ ë¶„ì„ ê²°ê³¼]")
#
#             # ì „ì²´ í‚¤ì›Œë“œ ë¹ˆë„ ë¶„ì„
#             all_keywords = []
#             for keywords in items_df['keywords']:
#                 if keywords:
#                     all_keywords.extend(keywords)
#
#             if all_keywords:
#                 from collections import Counter
#                 keyword_freq = Counter(all_keywords)
#                 top_keywords = keyword_freq.most_common(10)
#
#                 print(f"  ì¶”ì¶œëœ ì´ í‚¤ì›Œë“œ ìˆ˜: {len(set(all_keywords))}ê°œ")
#                 print(f"  ìƒìœ„ í‚¤ì›Œë“œ: {[kw for kw, freq in top_keywords[:5]]}")
#
#         # ì¶”ì²œ ì‹œìŠ¤í…œ ì¤€ë¹„ë„ í‰ê°€
#         print(f"\n[ì¶”ì²œ ì‹œìŠ¤í…œ ì¤€ë¹„ë„]")
#
#         if not items_df.empty and not members_df.empty and not interactions_df.empty:
#             # ë°ì´í„° í¬ì†Œì„±
#             total_combinations = len(members_df) * len(items_df)
#             actual_interactions = len(interactions_df.groupby(['user_id', 'product_id']).first())
#             sparsity = 1 - (actual_interactions / total_combinations)
#             print(f"  ë°ì´í„° í¬ì†Œì„±: {sparsity:.4f} ({sparsity*100:.2f}%)")
#
#             # ìµœì†Œ ìƒí˜¸ì‘ìš© ê¸°ì¤€
#             user_interaction_counts = interactions_df.groupby('user_id').size()
#             item_interaction_counts = interactions_df.groupby('product_id').size()
#
#             active_users = (user_interaction_counts >= 3).sum()
#             popular_items = (item_interaction_counts >= 2).sum()
#
#             print(f"  í™œì„± ì‚¬ìš©ì (3íšŒ ì´ìƒ): {active_users}/{len(members_df)} ({active_users/len(members_df)*100:.1f}%)")
#             print(f"  ì¸ê¸° ìƒí’ˆ (2íšŒ ì´ìƒ): {popular_items}/{len(items_df)} ({popular_items/len(items_df)*100:.1f}%)")
#
#             return {
#                 'items_count': len(items_df),
#                 'members_count': len(members_df),
#                 'interactions_count': len(interactions_df),
#                 'sparsity': sparsity,
#                 'active_users_ratio': active_users/len(members_df),
#                 'popular_items_ratio': popular_items/len(items_df),
#                 'total_keywords': len(set(all_keywords)) if 'all_keywords' in locals() and all_keywords else 0
#             }
#         else:
#             print("  ë°ì´í„°ê°€ ë¶€ì¡±í•˜ì—¬ ë¶„ì„í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
#             return {
#                 'items_count': len(items_df),
#                 'members_count': len(members_df),
#                 'interactions_count': len(interactions_df),
#                 'sparsity': 1.0,
#                 'active_users_ratio': 0.0,
#                 'popular_items_ratio': 0.0,
#                 'total_keywords': 0
#             }
#
#     def save_preprocessed_data(self, items_df, members_df, interactions_df, user_item_matrix, category_keywords=None, search_insights=None):
#         """ì „ì²˜ë¦¬ëœ ë°ì´í„° ì €ì¥ (NLP ê²°ê³¼ í¬í•¨)"""
#         try:
#             # íŒŒì¼ ê²½ë¡œ ì„¤ì •
#             if self.config:
#                 items_path = self.config.get_file_path('preprocessed_items.csv')
#                 members_path = self.config.get_file_path('preprocessed_members.csv')
#                 interactions_path = self.config.get_file_path('preprocessed_interactions.csv')
#                 matrix_path = self.config.get_file_path('user_item_matrix.csv')
#                 keywords_path = self.config.get_file_path('category_keywords.json')
#                 insights_path = self.config.get_file_path('search_insights.json')
#             else:
#                 items_path = 'preprocessed_items.csv'
#                 members_path = 'preprocessed_members.csv'
#                 interactions_path = 'preprocessed_interactions.csv'
#                 matrix_path = 'user_item_matrix.csv'
#                 keywords_path = 'category_keywords.json'
#                 insights_path = 'search_insights.json'
#
#             # CSV íŒŒì¼ë¡œ ì €ì¥
#             items_df.to_csv(items_path, index=False, encoding='utf-8')
#             members_df.to_csv(members_path, index=False, encoding='utf-8')
#             interactions_df.to_csv(interactions_path, index=False, encoding='utf-8')
#             user_item_matrix.to_csv(matrix_path, index=False, encoding='utf-8')
#
#             # NLP ë¶„ì„ ê²°ê³¼ ì €ì¥
#             if category_keywords:
#                 with open(keywords_path, 'w', encoding='utf-8') as f:
#                     json.dump(category_keywords, f, ensure_ascii=False, indent=2)
#
#             if search_insights:
#                 with open(insights_path, 'w', encoding='utf-8') as f:
#                     json.dump(search_insights, f, ensure_ascii=False, indent=2)
#
#             print(f"\nâœ… ì „ì²˜ë¦¬ëœ ë°ì´í„° ì €ì¥ ì™„ë£Œ!")
#             print(f"  - {items_path}: {len(items_df)}í–‰")
#             print(f"  - {members_path}: {len(members_df)}í–‰")
#             print(f"  - {interactions_path}: {len(interactions_df)}í–‰")
#             print(f"  - {matrix_path}: {len(user_item_matrix)}í–‰")
#
#             if category_keywords:
#                 print(f"  - {keywords_path}: {len(category_keywords)}ê°œ ì¹´í…Œê³ ë¦¬")
#             if search_insights:
#                 print(f"  - {insights_path}: ê²€ìƒ‰ ì¸ì‚¬ì´íŠ¸")
#
#         except Exception as e:
#             print(f"âŒ ë°ì´í„° ì €ì¥ ì‹¤íŒ¨: {e}")
#
# # ì‹¤í–‰ í•¨ìˆ˜
# def run_sonstar_data_preprocessing():
#     """SonStar ì‡¼í•‘ëª° ë°ì´í„° ì „ì²˜ë¦¬ ì‹¤í–‰ (NLP ê¸°ëŠ¥ í¬í•¨)"""
#
#     # 1. í”„ë¡œì„¸ì„œ ì´ˆê¸°í™”
#     processor = SonStarDataProcessor()
#
#     # 2. MySQL ë°ì´í„° ì¶”ì¶œ
#     mysql_data = processor.extract_data_from_mysql()
#     if mysql_data[0] is None:
#         print("MySQL ë°ì´í„° ì¶”ì¶œ ì‹¤íŒ¨. í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
#         return
#
#     # 3. Redis ë°ì´í„° ì¶”ì¶œ
#     redis_data = processor.extract_data_from_redis()
#
#     # 4. ë°ì´í„° í†µí•©
#     items_df, members_df, interactions_df = processor.merge_mysql_redis_data(mysql_data, redis_data)
#
#     # 5. ê° ë°ì´í„°ë³„ ì „ì²˜ë¦¬ (NLP í¬í•¨)
#     items_processed = processor.preprocess_items_data(items_df)
#     members_processed = processor.preprocess_members_data(members_df)
#     interactions_processed, user_item_matrix = processor.preprocess_interactions_data(interactions_df)
#
#     # 6. NLP ë¶„ì„ ìˆ˜í–‰
#     category_keywords = None
#     search_insights = None
#
#     if not items_processed.empty:
#         category_keywords = processor.analyze_category_keywords(items_processed)
#
#     if redis_data:
#         search_insights = processor.extract_search_insights(redis_data)
#
#     # 7. ìš”ì•½ í†µê³„ ìƒì„±
#     stats = processor.generate_summary_statistics(items_processed, members_processed, interactions_processed)
#
#     # 8. ì „ì²˜ë¦¬ëœ ë°ì´í„° ì €ì¥ (NLP ê²°ê³¼ í¬í•¨)
#     processor.save_preprocessed_data(
#         items_processed, members_processed, interactions_processed, user_item_matrix,
#         category_keywords, search_insights
#     )
#
#     print(f"\n SonStar ì‡¼í•‘ëª° ë°ì´í„° ì „ì²˜ë¦¬ ì™„ë£Œ! (NLP í‚¤ì›Œë“œ ì¶”ì¶œ í¬í•¨)")
#     print(f" NLP ì—”ì§„: {processor.nlp_engine}")
#
#     return items_processed, members_processed, interactions_processed, user_item_matrix, category_keywords, search_insights
#
# # ì‹¤í–‰ ì˜ˆì‹œ
# if __name__ == "__main__":
#     run_sonstar_data_preprocessing()